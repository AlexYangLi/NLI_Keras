# -*- coding: utf-8 -*-

"""

@author: alexyang

@contact: alex.yang0326@gmail.com

@file: cache.py

@time: 2019/3/13 22:11

@desc:

"""

import os
import struct
import hashlib
import lmdb
import pickle
import numpy as np
import tensorflow as tf
from allennlp.commands.elmo import ElmoEmbedder
from utils.bilm import Batcher, BidirectionalLanguageModel, weight_layers


class ELMoCache(object):
    """
    using LMDB, a key-value database to cache elmo embedding generated by a pre-trained elmo model
    """
    def __init__(self, options_file, weight_file, cache_dir, idx2token, max_sentence_length, elmo_model_type='allennlp',
                 vocab_file=None, elmo_output_mode='elmo'):
        """
        :param options_file: hyper-parameters of pre-trained elmo model
        :param weight_file: weights of pre-trained elmo model
        :param cache_dir: where to cache elmo embedding
        :param elmo_model_type: methods to load pre-trained elmo model and generate. We support 2 methods: 'allennlp'
                        (allennlp.commmands.elmo.ElmoEmbedder) or 'bilmtf'(tensorflow code of elmo implementation)
        :param vocab_file: vocab file with one token per line. Tokens in vocabulary will be used to cache 50
                           charcater id sequence for efficiency. Only apply when use 'bilmtf' as elmo_model_type
        :param elmo_output_mode: use which elmo output, options are 'word_embed', 'lstm_outputs1', 'lstm_outputs2',
                                 'elmo', 'default', only apply when use 'allennlp' to generate elmo embedding.
        """
        self.options_file = options_file
        self.weight_file = weight_file
        self.cache_dir = os.path.join(cache_dir, elmo_model_type, elmo_output_mode)
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        self.idx2token = idx2token
        self.max_sentence_length = max_sentence_length
        self.elmo_model_type = elmo_model_type
        self.vocab_file = vocab_file
        self.elmo_output_mode = elmo_output_mode
        self.embedding_size = 1024

        self.elmo_model = None
        self.init_elmo_model()

        self.elmo_env = None    # lmdb database to cache elmo embedding
        self.map_size = 100 * 1024 * 1024 * 1024    # default init size of a lmdb database for embeddings
        self.init_elmo_env()

    def init_elmo_model(self):
        if self.elmo_model_type == 'allennlp':
            print('Logging Info - Loading pre-trained elmo model using allennlp.ElmoEmbedder')
            self.elmo_model = ElmoEmbedder(options_file=self.options_file, weight_file=self.weight_file,
                                           cuda_device=0)
        elif self.elmo_model_type == 'bilmtf':
            print('Logging Info - Loading pre-trained elmo model using bilmtf')
            if self.vocab_file is None:
                self.vocab_file = os.path.join(self.cache_dir, 'vocab.txt')
                if not os.path.exists(self.vocab_file):
                    print('vocab_file input is None, we use idx2token to generate a file')
                    tokens = ['<S>', '</S>', '<UNK>']
                    tokens.extend(self.idx2token.values())
                    with open(self.vocab_file, 'w') as writer:
                        writer.write('\n'.join(tokens))

            self.graph = tf.Graph()
            with self.graph.as_default():
                # create a Batcher to map text to character ids
                self.batcher = Batcher(lm_vocab_file=self.vocab_file, max_token_length=50)
                # input placeholder to elmo model
                self.input_character_ids = tf.placeholder(tf.int32, (None, None, 50))
                # build the elmo graph
                self.elmo_model = BidirectionalLanguageModel(options_file=self.options_file, weight_file=self.weight_file)
                # get op to compute elmo embeddings
                self.embedding_op = self.elmo_model(self.input_character_ids)
        else:
            raise ValueError('Elmo model type `{}` not understood'.format(self.elmo_model_type))

    def init_elmo_env(self):
        self.elmo_env = lmdb.open(self.cache_dir, map_size=self.map_size)

    def convert_to_tokens(self, token_ids):
        return [self.idx2token.get(token_id) for token_id in token_ids if token_id in self.idx2token and token_id != 0]

    def embed_batch(self, batch_token_ids):
        batch_tokens = [self.convert_to_tokens(token_ids) for token_ids in batch_token_ids]

        # print('Logging Info - Try to get elmo embedding from lmda database')
        batch_elmo_embeddings = self.get_elmo_from_lmdb(batch_tokens)
        if batch_elmo_embeddings is None:
            # print('Logging Info - Get elmo embedding from lmda database falied')
            if self.elmo_model_type == 'allennlp':
                # print('Logging Info - Get elmo embedding from allennlp')
                batch_elmo_embeddings = self.get_elmo_from_allennlp(batch_tokens)
            elif self.elmo_model_type == 'bilmtf':
                # print('Logging Info - Get elmo embedding from bilmtf')
                batch_elmo_embeddings = self.get_elmo_from_bilmtf(batch_tokens)
            else:
                raise ValueError('elmo_model_type `{}` not understood'.format(self.elmo_model_type))
            # print('Logging Info - Cache generate elmo embedding')
            self.cache_elmo_to_lmdb(batch_tokens, batch_elmo_embeddings)
        # else:
            # print('Logging Info - Get elmo embedding from lmda database sucessfully')

        return batch_elmo_embeddings

    def get_elmo_from_lmdb(self, batch_tokens):
        """try to get elmo embedding from lmda database"""
        batch_elmo_embeddings = []

        try:
            txn = self.elmo_env.begin()
            for i in range(len(batch_tokens)):
                tokens = batch_tokens[i]
                token_hashed = self.list_digest(tokens)
                vector = txn.get(token_hashed.encode('utf8'))   # key must be byte
                if vector:
                    elmo_embedding = self.deserialize_pickle(vector)
                    batch_elmo_embeddings.append(elmo_embedding)
                else:
                    # print('Logging Info - can not get elmo embedding of {}'.format(batch_tokens[i]))
                    # if one sentence can't get its elmo embedding from cache, we return None immediately
                    return None
        except lmdb.Error:
            # no idea why, but we need to close and reopen the environment to avoid
            # mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot
            # when opening new transaction !
            # print('Logging Warning - LMDB ERROR, restart again')
            self.elmo_env.close()
            self.init_elmo_env()
            return self.get_elmo_from_lmdb(batch_tokens)
        # print('Logging Info - Get elmo embedding from lmda database sucessfully')
        return np.array(batch_elmo_embeddings)

    def cache_elmo_to_lmdb(self, batch_tokens, batch_elmo_embedding):
        if len(batch_tokens) != batch_elmo_embedding.shape[0]:
            raise ValueError('batch_tokens not equal to batch_elmo_embedding, got {} and {}'.format(len(batch_tokens),
                                                                                                    batch_elmo_embedding.shape[0]))
        txn = self.elmo_env.begin(write=True)
        for i in range(len(batch_tokens)):
            token_hash = self.list_digest(batch_tokens[i])
            txn.put(token_hash.encode('utf8'), self.serialize_pickle(batch_elmo_embedding[i]))
        txn.commit()

    def get_elmo_from_allennlp(self, batch_tokens):
        """input sentence are processed token id sequences"""
        embed_results = self.elmo_model.embed_batch(batch_tokens)
        batch_emlo_embeddings = np.array([self.pad_embedding(embed_result) for embed_result in embed_results])
        return batch_emlo_embeddings

    def get_elmo_from_bilmtf(self, batch_tokens):
        batch_character_ids = self.batcher.batch_sentences(batch_tokens)
        with tf.Session(graph=self.graph) as sess:
            sess.run(tf.global_variables_initializer())
            embed_results = sess.run(self.embedding_op['lm_embeddings'],
                                     feed_dict={self.input_character_ids: batch_character_ids})
            batch_emlo_embeddings = self.pad_embeddings(embed_results)
        return batch_emlo_embeddings

    def pad_embedding(self, embed_result):
        """pad or truncate one embedding"""
        if self.elmo_output_mode == 'word_embed':
            embedding = embed_result[0]
            time_dimension = 0
        elif self.elmo_output_mode == 'lstm_outputs1':
            embedding = embed_result[1]
            time_dimension = 0
        elif self.elmo_output_mode == 'lstm_outputs2':
            embedding = embed_result[2]
            time_dimension = 0
        elif self.elmo_output_mode == 'elmo_avg':
            embedding = np.average(embed_result, axis=0)
            time_dimension = 0
        elif self.elmo_output_mode == 'elmo':
            embedding = embed_result
            time_dimension = 1
        else:
            raise ValueError('Elmo output model `{}` not understood'.format(self.elmo_output_mode))

        if embedding.shape[time_dimension] > self.max_sentence_length:
            if time_dimension == 0:
                padded_embedding = embedding[:self.max_sentence_length, :]
            else:
                padded_embedding = embedding[:, :self.max_sentence_length, :]
        elif embedding.shape[time_dimension] < self.max_sentence_length:
            pad_length = self.max_sentence_length - embedding.shape[time_dimension]
            if time_dimension == 0:
                padded_embedding = np.concatenate((embedding, np.zeros(shape=(pad_length, self.embedding_size))), axis=0)
            else:
                padded_embedding = np.concatenate((embedding, np.zeros(shape=(3, pad_length, self.embedding_size))), axis=1)
        else:
            padded_embedding = embedding

        return padded_embedding

    def pad_embeddings(self, embed_results):
        """pad or truncate embedding matrix"""
        if self.elmo_output_mode == 'word_embed':
            embeddings = embed_results[:, 0, :, :]
            time_dimension = 1
        elif self.elmo_output_mode == 'lstm_outputs1':
            embeddings = embed_results[:, 1, :, :]
            time_dimension = 1
        elif self.elmo_output_mode == 'lstm_outputs2':
            embeddings = embed_results[:, 2, :, :]
            time_dimension = 1
        elif self.elmo_output_mode == 'elmo_avg':
            embeddings = np.average(embed_results, axis=1)
            time_dimension = 1
        elif self.elmo_output_mode == 'elmo':
            embeddings = embed_results
            time_dimension = 2
        else:
            raise ValueError('Elmo output model `{}` not understood'.format(self.elmo_output_mode))

        if embeddings.shape[time_dimension] > self.max_sentence_length:
            if time_dimension == 1:
                padded_embeddings = embeddings[:, self.max_sentence_length, :]
            else:
                padded_embeddings = embeddings[:, :self.max_sentence_length, :]
        elif embeddings.shape[time_dimension] < self.max_sentence_length:
            pad_length = self.max_sentence_length - embeddings.shape[time_dimension]
            if time_dimension == 1:
                padded_embeddings = np.concatenate((embeddings, np.zeros(shape=(embeddings.shape[0], pad_length,
                                                                                self.embedding_size))), axis=1)
            else:
                padded_embeddings = np.concatenate((embeddings, np.zeros(shape=(embeddings.shape[0], 3, pad_length,
                                                                                self.embedding_size))), axis=2)
        else:
            padded_embeddings = embeddings

        return padded_embeddings

    @staticmethod
    # use hashlib to encode a list of tokens
    def list_digest(strings):
        _hash = hashlib.sha1()
        for s in strings:
            _hash.update(struct.pack("I", len(s)))
            _hash.update(s.encode(encoding='UTF-8'))
        return _hash.hexdigest()

    @staticmethod
    def serialize_pickle(obj):
        return pickle.dumps(obj)

    @staticmethod
    def deserialize_pickle(serialized_obj):
        return pickle.loads(serialized_obj)













